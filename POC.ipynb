{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "from enum import Enum\n",
    "import openai\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "short-term memory is the record of the current chat,\n",
    "long-term memory could be indexed using a one-sentence summary of the entire chat,\n",
    "(short enough to be able to parse many without running out of tokens) \n",
    "and saved to a file externally.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Role = Enum('Role', ['system', 'user', 'assistant'])\n",
    "\n",
    "class ChatBot():\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = \"gpt-4\"\n",
    "        self._short_term_memory = []\n",
    "        self._system_prompt = None\n",
    "        \n",
    "        self._id = uuid.uuid4().hex\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        self._initialize()\n",
    "        self._program_loop()\n",
    "\n",
    "    def chat(self) -> None:\n",
    "        prompt_ = self._capture_user_text_input()\n",
    "        self._memorize_short_term(prompt_, Role.user)\n",
    "        raw_response_ = self._prompt_llm()\n",
    "        response_ = self._llm_output_to_text(raw_response_)\n",
    "        self._memorize_short_term(response_, Role.assistant)\n",
    "        self._output(response_, Role.assistant)\n",
    "\n",
    "    def forget_short_term_memory(self):\n",
    "        self._short_term_memory = []\n",
    "        if self._system_prompt is not None:\n",
    "            self._memorize_short_term(self._system_prompt, Role.system)\n",
    "\n",
    "    def show_short_term_memory(self) -> None:\n",
    "        print(self._short_term_memory)\n",
    "\n",
    "    def speak(self):\n",
    "        prompt_ = self._capture_user_speech_input()\n",
    "        if len(prompt_) == 0:\n",
    "            return\n",
    "        self._output(prompt_, Role.user)\n",
    "        self._memorize_short_term(prompt_, Role.user)\n",
    "        raw_response_ = self._prompt_llm()\n",
    "        response_ = self._llm_output_to_text(raw_response_)\n",
    "        self._memorize_short_term(response_, Role.assistant)\n",
    "        self._output(response_, Role.assistant)\n",
    "    \n",
    "    def _initialize(self):\n",
    "        sys_prompt_ = input(\"System prompt?\\n\")\n",
    "        if sys_prompt_ == \"\":\n",
    "            sys_prompt_ = None\n",
    "        if sys_prompt_ is not None:\n",
    "            self._system_prompt = sys_prompt_\n",
    "            self._memorize_short_term(sys_prompt_, Role.system)\n",
    "    \n",
    "    def _program_loop(self) -> None:\n",
    "        while (True):\n",
    "            input_ = input(\"Action? (C=chat, S=speak, F=forget, M=show memory, Q=quit)\\n\")\n",
    "            if input_ == \"C\":\n",
    "                self.chat()\n",
    "            elif input_ == \"S\":\n",
    "                self.speak()\n",
    "            elif input_ == \"F\":\n",
    "                self.forget_short_term_memory()\n",
    "            elif input_ == \"M\":\n",
    "                self.show_short_term_memory()\n",
    "            elif input_ == \"Q\":\n",
    "                break\n",
    "            else:\n",
    "                print(f'Invalid input: {input_}')\n",
    "    \n",
    "    def _memorize_short_term(self, memory_, role_) -> None:\n",
    "        self._short_term_memory += [{'role': role_.name, 'content': memory_}]\n",
    "\n",
    "    def _prompt_llm(self):\n",
    "        return openai.ChatCompletion.create(\n",
    "            model=self._model,\n",
    "            messages=self._short_term_memory\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _capture_user_text_input():\n",
    "        return input('User: ')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _capture_user_speech_input() -> str:\n",
    "        r = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening...\", flush=True)\n",
    "            r.adjust_for_ambient_noise(source, duration=0.2)\n",
    "            audio = r.listen(source)\n",
    "        # recognize speech using Whisper API\n",
    "        OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "        try:\n",
    "            prompt_ = r.recognize_whisper_api(audio, api_key=OPENAI_API_KEY)\n",
    "            return prompt_\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Whisper API.\")\n",
    "            print(e)\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _output(output_: str, role_: Role) -> None:\n",
    "        print(f'{role_.name}: {output_}', flush=True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _llm_output_to_text(output_) -> str:\n",
    "        if isinstance(output_, dict):\n",
    "            return output_[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only `str` is supported so far. Deal with voice later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ChatBot()\n",
    "c.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added an \"E=execute\" option to the Actions. It simply executes the LLM's output as a shell script.\n",
    "class Agent(ChatBot):\n",
    "    def _program_loop(self) -> None:\n",
    "         while (True):\n",
    "            input_ = input(\"Action? (C=chat, S=speak, F=forget, M=show memory, Q=quit)\\n\")\n",
    "            if input_ == \"C\":\n",
    "                self.chat()\n",
    "            elif input_ == \"S\":\n",
    "                self.speak()\n",
    "            elif input_ == \"E\":\n",
    "                self.execute()\n",
    "            elif input_ == \"F\":\n",
    "                self.forget_short_term_memory()\n",
    "            elif input_ == \"M\":\n",
    "                self.show_short_term_memory()\n",
    "            elif input_ == \"Q\":\n",
    "                break\n",
    "            else:\n",
    "                print(f'Invalid input: {input_}')\n",
    "\n",
    "    def execute(self):\n",
    "        last_memory_ = self._short_term_memory[-1]\n",
    "        if last_memory_['role'] == Role.assistant.name:\n",
    "            cmd_str_ = last_memory_['content']\n",
    "            subprocess.run(cmd_str_, shell=True)\n",
    "        else:\n",
    "            print(f\"Last memory must be of `assistant`, but is of role=`{last_memory_['role']}`\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent()\n",
    "a.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveAgent(Agent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelRecursiveAgent(RecursiveAgent):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
